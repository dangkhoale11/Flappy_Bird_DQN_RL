{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "592a737a-2c98-4d96-8924-985db658e9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from flappy_bird_env import FlappyBirdEnv\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "observation_size = 4\n",
    "action_size = 2\n",
    "learning_rate = 0.0001 \n",
    "batch_size = 128\n",
    "memory_capacity = 50000\n",
    "\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "538b27d2-e7db-4c39-a1f4-101c7cf52412",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "        \n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3cd0b4f-adf3-47db-90e0-78fac685ce0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNnetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(observation_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "        \n",
    "        # Khởi tạo trọng số\n",
    "        nn.init.kaiming_normal_(self.fc1.weight)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight)\n",
    "        nn.init.xavier_normal_(self.fc3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b673daec-c00b-4a3e-9b36-c0c96c22227c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.gamma = 0.995 #discount factor\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.995  # chậm hơn để khám phá nhiều hơn ban đầu\n",
    "        self.epsilon_min = 0.005   # vẫn cho phép một chút khám phá sau này\n",
    "        self.target_update = 10  # Update target network every N episodes\n",
    "\n",
    "        self.memory = ReplayMemory(memory_capacity)\n",
    "        #policy network\n",
    "        self.policy_net = DQNnetwork().to(device)\n",
    "        #target y for trainning\n",
    "        self.target_net = DQNnetwork().to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()  # Target network is in evaluation mode\n",
    "\n",
    "        # Define optimizer\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)\n",
    "\n",
    "    def act(self, state):\n",
    "        # Epsilon-greedy action selection\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(action_size)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            q_values = self.policy_net(state)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.push(\n",
    "            torch.FloatTensor(state).to(device),\n",
    "            torch.tensor([action]).to(device),\n",
    "            torch.FloatTensor(next_state).to(device) if not done else None,\n",
    "            torch.tensor([reward], dtype=torch.float32).to(device),\n",
    "            torch.tensor([done], dtype=torch.bool).to(device)\n",
    "        )\n",
    "\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "    \n",
    "        batch_data = self.memory.sample(batch_size)\n",
    "        batch_data = Transition(*zip(*batch_data))\n",
    "    \n",
    "        # Tách dữ liệu\n",
    "        batch_state = torch.stack(batch_data.state).to(device)                 # [batch_size, state_dim]\n",
    "        batch_action = torch.cat(batch_data.action).unsqueeze(1).to(device)   # [batch_size, 1]\n",
    "        batch_reward = torch.cat(batch_data.reward).to(device)                # [batch_size]\n",
    "        \n",
    "        # Xử lý next_state\n",
    "        non_final_mask = ~torch.tensor(batch_data.done, dtype=torch.bool, device=device)\n",
    "        non_final_next_states = torch.stack([s for s, d in zip(batch_data.next_state, batch_data.done) if not d]).to(device)\n",
    "    \n",
    "        # Tính giá trị Q hiện tại theo hành động đã chọn\n",
    "        state_action_values = self.policy_net(batch_state).gather(1, batch_action)  # [batch_size, 1]\n",
    "    \n",
    "        # Tính Q mục tiêu\n",
    "        next_state_values = torch.zeros(batch_size, device=device)\n",
    "        if non_final_mask.any():\n",
    "            with torch.no_grad():\n",
    "                next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0]\n",
    "    \n",
    "        expected_state_action_values = batch_reward + self.gamma * next_state_values\n",
    "    \n",
    "        # Tính loss\n",
    "        loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "        # Tối ưu\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.policy_net.parameters():\n",
    "            if param.grad is not None:\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "    \n",
    "        # Giảm epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        \n",
    "\n",
    "    \n",
    "    def save(self, filename):\n",
    "        torch.save({\n",
    "            'policy_model': self.policy_net.state_dict(),\n",
    "            'target_model': self.target_net.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'epsilon': self.epsilon\n",
    "        }, filename)\n",
    "        print(f\"Model saved to {filename}\")\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "    \n",
    "    def load(self, filename):\n",
    "        if os.path.isfile(filename):\n",
    "            checkpoint = torch.load(filename)\n",
    "            self.policy_net.load_state_dict(checkpoint['policy_model'])\n",
    "            self.target_net.load_state_dict(checkpoint['target_model'])\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            self.epsilon = checkpoint['epsilon']\n",
    "            print(f\"Model loaded from {filename}\")\n",
    "        else:\n",
    "            print(f\"No model found at {filename}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a461bef-5c04-4614-9f2d-15ac551b8a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn(episodes=1000, render_freq=100):\n",
    "    \"\"\"Train the DQN agent\"\"\"\n",
    "    env = FlappyBirdEnv(render_mode=\"human\" if render_freq > 0 else None)\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    \n",
    "    agent = DQNAgent()\n",
    "    scores = []\n",
    "    epsilon_history = []\n",
    "    \n",
    "    for e in range(1, episodes + 1):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        # Set render mode for visualization\n",
    "        if e % render_freq == 0:\n",
    "            env = FlappyBirdEnv(render_mode=\"human\")\n",
    "        else:\n",
    "            env = FlappyBirdEnv(render_mode=None)\n",
    "        \n",
    "        while not done:\n",
    "            # Select action\n",
    "            action = agent.act(state)\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Remember experience\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Update state and reward\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Train the network\n",
    "            agent.replay()\n",
    "        \n",
    "        # Update target network\n",
    "        if e % agent.target_update == 0:\n",
    "            agent.update_target_network()\n",
    "        \n",
    "        # Save scores and epsilon\n",
    "        scores.append(total_reward)\n",
    "        epsilon_history.append(agent.epsilon)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Episode: {e}/{episodes}, Score: {total_reward:.2f}, Epsilon: {agent.epsilon:.4f}\")\n",
    "        \n",
    "        # Save model periodically\n",
    "        if e % 100 == 0:\n",
    "            agent.save(f\"flappy_bird_dqn_ep{e}.pth\")\n",
    "    \n",
    "    # Save final model\n",
    "    agent.save(\"flappy_bird_dqn_final.pth\")\n",
    "    \n",
    "    # Plot training results\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(scores)\n",
    "    plt.title('Score per Episode')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Score')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epsilon_history)\n",
    "    plt.title('Epsilon per Episode')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Epsilon')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('flappy_bird_dqn_training.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b19af26-290a-4d4a-a314-461de93bb2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/1000, Score: 21.53, Epsilon: 1.0000\n",
      "Episode: 2/1000, Score: 19.81, Epsilon: 0.9607\n",
      "Episode: 3/1000, Score: 19.54, Epsilon: 0.6798\n",
      "Episode: 4/1000, Score: 26.19, Epsilon: 0.4529\n",
      "Episode: 5/1000, Score: 31.80, Epsilon: 0.2973\n",
      "Episode: 6/1000, Score: 48.40, Epsilon: 0.1489\n",
      "Episode: 7/1000, Score: 45.26, Epsilon: 0.0849\n",
      "Episode: 8/1000, Score: 29.91, Epsilon: 0.0635\n",
      "Episode: 9/1000, Score: -15.24, Epsilon: 0.0563\n",
      "Episode: 10/1000, Score: -9.33, Epsilon: 0.0499\n",
      "Episode: 11/1000, Score: -1.27, Epsilon: 0.0443\n",
      "Episode: 12/1000, Score: 57.74, Epsilon: 0.0218\n",
      "Episode: 13/1000, Score: 176.31, Epsilon: 0.0069\n",
      "Episode: 14/1000, Score: 107.57, Epsilon: 0.0050\n",
      "Episode: 15/1000, Score: 1362.92, Epsilon: 0.0050\n",
      "Episode: 16/1000, Score: 57.49, Epsilon: 0.0050\n",
      "Episode: 17/1000, Score: 717.38, Epsilon: 0.0050\n",
      "Episode: 18/1000, Score: 243.02, Epsilon: 0.0050\n",
      "Episode: 19/1000, Score: 73.07, Epsilon: 0.0050\n",
      "Episode: 20/1000, Score: 240.17, Epsilon: 0.0050\n",
      "Episode: 21/1000, Score: 159.06, Epsilon: 0.0050\n",
      "Episode: 22/1000, Score: 1154.09, Epsilon: 0.0050\n",
      "Episode: 23/1000, Score: 7562.13, Epsilon: 0.0050\n",
      "Episode: 24/1000, Score: 260.02, Epsilon: 0.0050\n",
      "Episode: 25/1000, Score: 695.72, Epsilon: 0.0050\n",
      "Episode: 26/1000, Score: 188.93, Epsilon: 0.0050\n",
      "Episode: 27/1000, Score: 842.47, Epsilon: 0.0050\n",
      "Episode: 28/1000, Score: 187.55, Epsilon: 0.0050\n",
      "Episode: 29/1000, Score: 1178.05, Epsilon: 0.0050\n",
      "Episode: 30/1000, Score: 220.85, Epsilon: 0.0050\n",
      "Episode: 31/1000, Score: 136.84, Epsilon: 0.0050\n",
      "Episode: 32/1000, Score: 3520.52, Epsilon: 0.0050\n",
      "Episode: 33/1000, Score: 505.88, Epsilon: 0.0050\n",
      "Episode: 34/1000, Score: 1104.51, Epsilon: 0.0050\n",
      "Episode: 35/1000, Score: 243.65, Epsilon: 0.0050\n",
      "Episode: 36/1000, Score: 477.87, Epsilon: 0.0050\n",
      "Episode: 37/1000, Score: 324.78, Epsilon: 0.0050\n",
      "Episode: 38/1000, Score: 465.86, Epsilon: 0.0050\n",
      "Episode: 39/1000, Score: 241.81, Epsilon: 0.0050\n",
      "Episode: 40/1000, Score: 707.88, Epsilon: 0.0050\n",
      "Episode: 41/1000, Score: 663.76, Epsilon: 0.0050\n",
      "Episode: 42/1000, Score: 429.40, Epsilon: 0.0050\n",
      "Episode: 43/1000, Score: 521.33, Epsilon: 0.0050\n",
      "Episode: 44/1000, Score: 963.24, Epsilon: 0.0050\n",
      "Episode: 45/1000, Score: 438.52, Epsilon: 0.0050\n",
      "Episode: 46/1000, Score: 239.84, Epsilon: 0.0050\n",
      "Episode: 47/1000, Score: 1424.97, Epsilon: 0.0050\n",
      "Episode: 48/1000, Score: 1024.57, Epsilon: 0.0050\n",
      "Episode: 49/1000, Score: 240.76, Epsilon: 0.0050\n",
      "Episode: 50/1000, Score: 454.66, Epsilon: 0.0050\n",
      "Episode: 51/1000, Score: 533.58, Epsilon: 0.0050\n",
      "Episode: 52/1000, Score: 976.42, Epsilon: 0.0050\n",
      "Episode: 53/1000, Score: 229.40, Epsilon: 0.0050\n",
      "Episode: 54/1000, Score: 1074.58, Epsilon: 0.0050\n",
      "Episode: 55/1000, Score: 925.04, Epsilon: 0.0050\n",
      "Episode: 56/1000, Score: 1825.34, Epsilon: 0.0050\n",
      "Episode: 57/1000, Score: 458.47, Epsilon: 0.0050\n",
      "Episode: 58/1000, Score: 1260.31, Epsilon: 0.0050\n",
      "Episode: 59/1000, Score: 484.32, Epsilon: 0.0050\n",
      "Episode: 60/1000, Score: 510.85, Epsilon: 0.0050\n",
      "Episode: 61/1000, Score: 3983.63, Epsilon: 0.0050\n",
      "Episode: 62/1000, Score: 7477.91, Epsilon: 0.0050\n",
      "Episode: 63/1000, Score: 23694.66, Epsilon: 0.0050\n",
      "Episode: 64/1000, Score: 7359.91, Epsilon: 0.0050\n"
     ]
    }
   ],
   "source": [
    "agent = train_dqn(episodes=1000, render_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803bf4a6-75d2-46cf-9324-09266fcaf271",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caffec5-605c-4b43-9bdc-635dc445766d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (RL venv)",
   "language": "python",
   "name": "rl-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
